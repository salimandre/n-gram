# N-gram models for language modeling

We simply built n-gram models which are simple baseline language models. More precisely we built bigram and trigram models. Raw n-gram cannot be evaluated as they overfit training text data too much. Hence it is required to use a smoothing, regularization technique. We chose an intuitive simple one by doing computing the mixture between an ngram model and the uniform distribution over vocabulary.

https://www.gutenberg.org/ebooks/search/%3Fsort_order%3Ddownloads
